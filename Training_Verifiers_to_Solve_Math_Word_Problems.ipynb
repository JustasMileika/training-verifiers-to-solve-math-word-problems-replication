{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iTXr0J1b8oq"
   },
   "source": [
    "# About\n",
    "\n",
    "This is a notebook attempting to reproduce the OpenAI paper \"Training Verifiers to Solve Math Word Problems\" https://arxiv.org/abs/2110.14168\n",
    "\n",
    "So far only the generator training part is done. The verifier training should be done next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSDwEnqiIKOR"
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FEICOU3zBt5",
    "outputId": "35feeba3-0f41-4fbe-9fb5-dc1824245701"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "77SzeodpetGS",
    "outputId": "db1c240d-be48-40ce-e5c8-6b8e8dad4f5a"
   },
   "outputs": [],
   "source": [
    "%autosave 60\n",
    "\n",
    "!pip install datasets transformers\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INo7x3Vze765"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, \\\n",
    "    DataCollatorWithPadding, DataCollatorForLanguageModeling, default_data_collator, DataCollatorForSeq2Seq, \\\n",
    "    EarlyStoppingCallback, AutoModelForSequenceClassification, TrainerCallback\n",
    "from transformers import trainer_utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 123  # for results reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MH-OBEDEfClu",
    "outputId": "360acbdf-b1a5-4c20-a0ea-dba7c7e8522f"
   },
   "outputs": [],
   "source": [
    "hf_token = \"<hf_token>\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "wandb.login(key=\"<wandb_token>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483,
     "referenced_widgets": [
      "c1693c32b9d24d008e28837b938e8969",
      "051ae34409b743b59f54b62be5c0df2d",
      "4be5fb874439413990f94ead34ab8e4a",
      "cbb6131d89724f6b939f6bd5eff1310c",
      "0817b10606e741f8aacd1e065ee197b6",
      "8ac8b1eec9294cac8576cf0d141a9c4c",
      "66d153ee3f7b4e2e8f60cd3d7f121beb",
      "a9dc0fa201ec417f84561e68561202c4",
      "966a09513b644e92b075241b87912f08",
      "2f26c8050b634f7ca7d3a76f79b06ab7",
      "73577a930b5e4798adae996028f08792",
      "5607d4cf3f4e407da632cc6c591045fd",
      "218390db85b5468d891adccc58a18094",
      "d7eb90b5d84a473f99a3411280d049df",
      "d17677a4754f441bb23a9e9f79695edf",
      "3261804d8df44f3d94a9025968d0b7cd",
      "0de68c6700b946bebcdc873d394e8570",
      "fa2b6fb3bda641b7a0c0605dbd6c0f4b",
      "becd3cfcecac4c07ae0b830e54f28efe",
      "2d37cfda4972423399a494e0285061fe",
      "761bf619935f4c069e704adf69da5fbf",
      "415052fbd39b470fb02a6677a877f88c",
      "0e5c9d5ad91a40eda6931d49af2e6488",
      "721a78f5e52c4cd8ae60a19970eef3f5",
      "ebba760f4dea4f09a04c31982e3dbe66",
      "8273ebff9867404b90e453a062753044",
      "204cd23765b84186acc8f791fea88459",
      "d270280100874b1d88ac796d28c9c3ad",
      "595cb20e1f9d40119012becfbb8ba569",
      "205b69013aed4da3a6071ab66e5d2cbe",
      "bd5e93754e6f447dbf1d12936153a205",
      "7a2a93ae7fff42be8b0924ddc00ac4fc",
      "5a9f113eb6ed43dfb68f69c643347fd1",
      "e07640b1d69846628eb5a2d6ad91e808",
      "39b2ce6cec724075885ffa1b785ced85",
      "10731ebe463d4dd0bfc1e36f81ac537f",
      "ce09afc5424f426399acbd3ba2f1ca49",
      "9fa3130aa4c045fbaf44463f93198af7",
      "7aa44fd675184a189cfb304dc0be6804",
      "a79865ca5283430eb7ade59e81a9c8df",
      "24ffe2ce144d4109b0522bf83d2d5f64",
      "dcb7071fb3934e15945c899d2a0e1c25",
      "8af8c300e92a4f259cc7c16d352099a3",
      "1fcbd8e3093742a8b68d21d406a3ab45",
      "de0eb0832a4e425bb12136386b955b82",
      "c7102c16ab734aa68753b5c18723fae7",
      "b9313ba0a53b4586a7693b7866e994bc",
      "8392517392f84a0d9bd105877acf18f9",
      "72efd47e5da745ba9f7ac5f958c12007",
      "45090d5110d348dc97512e103869ab6a",
      "e378422a4db54a71a1d51cb2a5df3a81",
      "dfed273426b543549dc13c3d4cefd707",
      "86cb495d679848c7b77af870634bee20",
      "c9653587718643348a04d6794f75cba4",
      "b891d5027b73425d825f9cde444138ed"
     ]
    },
    "id": "gbxee3mRfGXC",
    "outputId": "12852b34-ffb5-4cc6-d1d1-4e82ecf3c668"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Justelioo/gsm8k-cleaned\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEY-52W8fNhm",
    "outputId": "11a40ab4-ede9-40ea-f477-753220b95c11"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpUCLY93fNuD"
   },
   "outputs": [],
   "source": [
    "def extract_final_answer(model_output):\n",
    "    matches = re.findall(r\"####\\s*\\$?([0-9,.]+)\", model_output)\n",
    "    if matches:\n",
    "        answer = matches[-1].replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(answer)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyf2Z2gwIR0N"
   },
   "source": [
    "#Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDkABeBcDcoS"
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"meta-llama/Llama-3.2-3B\"\n",
    "MODEL_SAVE_PATH = \"drive/MyDrive/gsm8k-generators/llama-3.2-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "971ecaec29ac4015bea14b62331e9cf6",
      "2925e3cfd670438daab37d0b56631813",
      "08072cd0bba84bfaa88cfd321543254a",
      "a44da39afe86461486825492ffe54af5",
      "0a3bb56ccd044fa68c0988e7ce8f058a",
      "1fcc51ab2e7c4b2fa2b03e65e1c617e5",
      "cd7324c559f7406090d9129979cf70c9",
      "d2173490498e461e8672eb17714487aa",
      "3dc90943219d4a08a1282c7517c19184",
      "06178d69cc0b4bbea27d8b1c5990ea2c",
      "ea43212724504d348ac7c84252d6f890",
      "e17e686e456f4ece8fa967b8687cabff",
      "b0c683f4904449d5aa64ef6bf9541c4d",
      "6e5c87c2d57c470cba5a8580b6392e53",
      "e61585dc0e9a44da9065501670409de4",
      "16aed27ba03e4ed1aa70d204e31ac53b",
      "cb68160f68674adda385610b9aa69def",
      "8068d9be80a14c3d9b0e3b1c69bdc7d9",
      "34e7352af4ef4b7ebad589faeadbbfc2",
      "95c74354e1aa4c3c8982ac9414a5cc64",
      "26929c1860364f6eb6175db27f47b7a6",
      "09890b9f7d17454f91d5bb60c2f9aba6",
      "f071b500de1a49a09193a1e0a757485f",
      "32d7af38e7e1482f988a55ba8f676e88",
      "449564369350439db82c72260a5f2d47",
      "7e8748eee9644ff89c0e6be0aadbb7d1",
      "eae660cb0e6747fdb3030a863288da02",
      "428ba22904894b189b6af6de00085f6a",
      "57727b5d51c845dba604240f48c8383a",
      "e2cd1f98a0194644b23bb10de21bf54f",
      "7fa766ad58904d3a9658f5a5d4629567",
      "d30941dd8ee54ace89a2e5ee979248bf",
      "8e6d5c0344d846079d91720549d09df6"
     ]
    },
    "id": "aRo1VLfjDlJ4",
    "outputId": "9e313148-e2f2-4081-899c-df5bf3fc0311"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "c113392c66264e5c945a027da3221eea",
      "044917451b814cd4b593fb5157f05197",
      "64ecbc932c464ad1964eb1cb93d30a64",
      "6cfd2ed6f2c64f1b80c9ab680c4caf37",
      "039d8a6691b945bbae926e2755fc5d72",
      "f211870095b34729a4bd4b43adb30a33",
      "1ed6f8d38c3e4d30b9c1af52be5b1cd3",
      "2b527f4c23e24d25941536e74d2aa68d",
      "37d9e80f549f4fe7a51b7822c3ff6710",
      "45e4d4a9f2504079917e8bcc30e647d7",
      "7a1282b038944befaa55c0bc373ed8b9",
      "c2b1fcdfc3bc450d97a4aa9389dae120",
      "c334fbecca904d9c879527511a6ec694",
      "8c0db292499a4d7ba70159fa9910b78c",
      "548772fa50b84507b699fee94959afff",
      "fe6f6440d1e24099a4580e080c24fffe",
      "2f18c32081fb4a16a64bf008434bf117",
      "a2a7b99e4c7a450eb6f851997d7052d9",
      "139061dafb844a928a385ab7409f96e8",
      "f2fc993ecbad44a797ebdd215a3a782c",
      "b5b654f30616490895b93a98742e5b7c",
      "93e3ad2f5ce74236abddb9546a66fd55",
      "084cde9999954d1e87843e30ca2d3cbe",
      "d7f9fbf8813845868756820b23561feb",
      "189413bd6e78454c86e6239235c0280c",
      "2fc84d2ad9234d7ca3840b12a6ea833d",
      "9a10d0d7d64a49fbb351d66a08165f95",
      "ea07792eedbe410881732ba6820ae807",
      "6206cb461ed140b5bd325f674e344f66",
      "6601625bfb634ac1aff1b24c11c3f2db",
      "233236029c984788975a08b5b5a00619",
      "69d2a027247442a2b193c21bd3aa9931",
      "2605c88da50f4edc9125f3832ba8b5d1",
      "0e9fea52d8b84ad4aa31b2a643a43366",
      "52ee121a5a3c4b10a290bfb18dfcc1cd",
      "f22712763e154e0f8838e784c3d1c304",
      "02ee39bcc1d14a9d8530ce530143071b",
      "425bd3ffd16743a5a207be7df3c94fd7",
      "694e8354e9cf448da252cf80f5c9d0da",
      "57e1fc579630484c9bf5949d936179da",
      "c15b1e7cd26040c5aef7c43138348042",
      "29f1a21aebb54b738fa53a00e58e1f96",
      "0fe979e5e2f149c8ac1af9e637ff57a3",
      "bb580ec7577b4e23b36990cbf90fcaaa",
      "58c026e72a884b3681a3dc510c7ab04b",
      "e04a8cdda68542b8b59ea61799dcffe0",
      "ffa180a1bdf44e43bc3dd97e9a8eb956",
      "8be974256d284b92ba7078507f68aeb1",
      "005af0c2e0c74891acd0b29f1827b82d",
      "3f149ee490c245d6afd06a44a96af8b7",
      "cf98e4d16d9d4b169ab6f3b88dac986f",
      "c7de2bf33f8e449d908344bff5e6eb9a",
      "2eb792c1a9b74999b7ab8f224f7d7c9a",
      "d1a4c8f4e116499d880340df6b4febbd",
      "b00f1b57b0564685a361f2ae6cbd1a2a",
      "34d9a12e2dd748fea3d9a7dafc48a4a2",
      "9a468973bf6f4acb808d160a8bb3e236",
      "a7cc72fadc694e9dac15b597c6935e9e",
      "5ee7ef7c75314979ad1d14bdec90a884",
      "ee22b66f502146fe82a6f850303d29c6",
      "87207d07c91746728ca0b92eed98291c",
      "14afee511daa46098b036f7e0732d198",
      "416867032148401b9241416717d6a595",
      "aac487245ea542b3a32de59b2eca7a19",
      "25758375608f432abc6e3d0ba76cee94",
      "eb895e55e8f34ec98632c958b0bf0899",
      "738b67e290f149d0bfcd2f21473f5f88",
      "e12c6d2447c445fe945563db333a6da3",
      "3d1a1f62a92e434894f1c2b2c2a5e6cd",
      "fc3983d134fa4b9d9d828270539a3011",
      "b9b78341f1f948b6a543b16ede054ced",
      "1f92dedbdb784f21811f6764841a2e25",
      "61f62183d2b947e0b787a5d1b650bf96",
      "f51094b930e0415c9492198da8ba45c1",
      "786e275dcb3045c187204b05d3e6685c",
      "08154efe5ea9482f8ee15b530731d812",
      "d2a3f117de4d46a9afd1a3713aeac877"
     ]
    },
    "id": "QoQ2zHnSIyOn",
    "outputId": "40ac57dd-8051-4b05-aeda-1201e532e042"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT, device_map=\"cuda\", dtype=torch.bfloat16)\n",
    "model.config.pad_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQQoSgTIJ0q2",
    "outputId": "c5b4ff5c-3336-49bb-f091-25b68348cbb8"
   },
   "outputs": [],
   "source": [
    "print(next(model.parameters()).dtype)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtxIsAi6MNZx"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You will be given a math question.\n",
    "You have to solve the question and provide the solution and the result.\n",
    "The result MUST be given after the symbols '####'.\"\"\"\n",
    "\n",
    "def qa_prompt(question, answer):\n",
    "  return f\"{SYSTEM_PROMPT}\\nQuestion:\\n{question}\\nAnswer:\\n{answer}\\n\"\n",
    "\n",
    "def q_prompt(question):\n",
    "  return f\"{SYSTEM_PROMPT}\\nQuestion:\\n{question}\\nAnswer:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZhs0oX2e2rx",
    "outputId": "0069c76c-e47f-4132-c149-338719378cbe"
   },
   "outputs": [],
   "source": [
    "print(qa_prompt(\"this is some question\", \"this is the answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBCjblDUIZwk"
   },
   "source": [
    "#Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHsMwBcSJzQy"
   },
   "outputs": [],
   "source": [
    "def prepare_training_text(example):\n",
    "  questions = example[\"question\"]\n",
    "  answers = example[\"answer\"]\n",
    "\n",
    "  texts = []\n",
    "\n",
    "  for q, a in zip(questions, answers):\n",
    "    text = qa_prompt(q, a)\n",
    "    texts.append(text)\n",
    "\n",
    "  return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "09bfddfc0ab440de90314230db02cde6",
      "dfee85269cd8458f99b02ea404f20ac2",
      "12947f7a69ce4257a7ba7de6c5fc1301",
      "53e3c5d2a5464d95b6c02db2e0ee6aaa",
      "e9a2f5021afa45b0b11e181a915deaf0",
      "5f8e02c8340a463194b9bee79c704d4b",
      "98679288023140f08e5c42101f2fd797",
      "1eae929e209441b3befd21977d50a7f1",
      "365af07f748b4c75ba1e8a66c1ce5de1",
      "f249bd6ee6a045f2b109d0c743d3f8a7",
      "6447f672351a42d78906de8a828fd7da",
      "0fdfe3e08bef4dd3ac15985b2b1c9ed2",
      "5e34b7a0f00b4f55a508bc0b61102095",
      "2cc089b7a4fd4090b6df2728a3698d84",
      "65169910897b425fadbc4f318cb9b0b1",
      "9ab26ae0806f41729075c07aecb94622",
      "fa9062ab3a03493bba39bf46fed80f46",
      "95fe3975ed5a487fbc68d100801b289b",
      "f9856f9180294e79bfcb7ba90a231e10",
      "eb4d6955d4e649a29229bdd956145c5f",
      "84f49d67c53948cb87ff96a1c53fd0a5",
      "de9829fc09df4f8683afa33c6b6e67ea"
     ]
    },
    "id": "Pvz2byGMKmH_",
    "outputId": "c10c7c69-6a5f-44e4-c033-3c3f2651f735"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_training_text, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QB-rnBKLfCmi",
    "outputId": "5eb3c373-be9f-446a-8edd-69b9ffd4798b"
   },
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yG2sHo-MsUpM",
    "outputId": "88b8d8da-1557-4fb0-d07f-555c67d96fdd"
   },
   "outputs": [],
   "source": [
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=123).select(range(32*4))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpNHZ1D4A3Sf"
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "  question_prefix = q_prompt(example[\"question\"])\n",
    "  text = example[\"text\"] + tokenizer.eos_token\n",
    "\n",
    "  tokenized = tokenizer(text, add_special_tokens=True)\n",
    "\n",
    "  input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "  prompt_enc = tokenizer(question_prefix, add_special_tokens=True)\n",
    "  prompt_len = len(prompt_enc[\"input_ids\"])\n",
    "\n",
    "  labels = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "\n",
    "  tokenized[\"labels\"] = labels\n",
    "\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295,
     "referenced_widgets": [
      "e957dd8ef74b42adbbfb59942d7a7233",
      "a1219c6b6fc741f7b0bc6cdd195f9ea7",
      "0100d8b6595e42ee928470a4133562f0",
      "9305aca7c94247bfbcc0909d336ed0ab",
      "f1a0308c14944936b2aaa40e51c956d5",
      "ea6326ba6b1a423d9d9fcae0fd838f71",
      "4f6f970e83cb460483e4ac8973ade0ea",
      "a5038b53f82b4ed38f5df8cae82a82fd",
      "78cd8613e6e848e0a7943562bdfb5b3a",
      "da6d45769a804b8da78e87dfa4c55b34",
      "a4ab83edbec446ba970fa5f38e06dd77",
      "6b03affc696948a698ed8b878c1e57f8",
      "fd442dab0c5848818cc2be71490612ad",
      "2406076449b243ca83b3ae0cbef98fbc",
      "e412b1038bda4ffaa63fc61e69f4001c",
      "d73e2d879bec41a995f63580f294cdd8",
      "0ae39fef658840d39ae1d9daaa9f7cfe",
      "dda053530ba2428299e975457bba906d",
      "91436303e6954915ac5d3904f480fd34",
      "7bff1eb7b9d8433282d15b4f70dec7f3",
      "d578bcff9c2745a9be31e3bc8fdb71f6",
      "a9430ddbdcdc46c7985eaed22a429f07"
     ]
    },
    "id": "VQTh7lmENQDe",
    "outputId": "c3936c49-eef4-4e57-8adc-52541e8c8e79"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize, batched=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS62ZwPNPbr9",
    "outputId": "c0d6fb6f-5183-449e-9958-e75008d36367"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for input_id, label in zip(dataset[\"train\"][\"input_ids\"][idx], dataset[\"train\"][\"labels\"][idx]):\n",
    "  print(input_id, label, tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOeAvPfnIcoa"
   },
   "source": [
    "#Load model for QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfq9fvZDHszu"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6Dj5KZvIqrc",
    "outputId": "1240510b-ece1-42cf-f052-1d4833b35dbf"
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik6QMaLlIhx1"
   },
   "source": [
    "#Functions for evaluation metrics\n",
    "\n",
    "pass@$1$ and pass@$n$ are reported, which represent if the problem was solved at least one time using $1$ and $n$ tries respectively.\n",
    "\n",
    "pass@1 is done with temperature=0.\n",
    "\n",
    "pass@n is done with temperature=0.7\n",
    "\n",
    "pass@$n$ is run with $n=16$ (in the original paper $n=100$, but due to computational restrictions, lower $n$ is chosen)\n",
    "\n",
    "pass@$n$ is run two times and both runs are reported to see the variation of different runs, since temperature is not 0\n",
    "\n",
    "the evaluation set consists of only 128 problems, again, due to computational restrictions\n",
    "\n",
    "evaluation is done using batches for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sIz1dbAPjiU"
   },
   "outputs": [],
   "source": [
    "def generate_without_sampling(model, tokenized):\n",
    "  with torch.no_grad():\n",
    "      results = model.generate(\n",
    "          **tokenized,\n",
    "          max_new_tokens=400,\n",
    "          do_sample=False,\n",
    "          use_cache=True,\n",
    "      )\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5r3TY4Hjl50"
   },
   "outputs": [],
   "source": [
    "def generate_with_sampling(model, tokenized, temperature, n_solutions):\n",
    "  with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **tokenized,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        use_cache=True,\n",
    "        num_return_sequences=n_solutions,\n",
    "    )\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rck8FOuQCtji"
   },
   "outputs": [],
   "source": [
    "def eval_pass_at_1(model, questions, true_answers, problems_per_batch=32):\n",
    "  generated_answers = []\n",
    "  for start in tqdm(range(0, len(questions), problems_per_batch)):\n",
    "    batch_questions = questions[start:start + problems_per_batch]\n",
    "\n",
    "    prompts = [q_prompt(q) for q in batch_questions]\n",
    "\n",
    "    tok_q = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    results = generate_without_sampling(model, tok_q)\n",
    "\n",
    "    batch_answers = tokenizer.batch_decode(\n",
    "        results,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    generated_answers.extend(batch_answers)\n",
    "\n",
    "    correct_count = count_correct_answer_count(generated_answers, true_answers)\n",
    "\n",
    "  return correct_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77Cnuz325CMr"
   },
   "outputs": [],
   "source": [
    "def is_correct(generated_answer, true_answer):\n",
    "  extracted_true_answer = extract_final_answer(true_answer)\n",
    "  extracted_generated_answer = extract_final_answer(generated_answer)\n",
    "  return extracted_generated_answer == extracted_true_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m4e5KAp5CMs"
   },
   "outputs": [],
   "source": [
    "def count_correct_answer_count(generated_answers, true_answers):\n",
    "  correct_count = 0\n",
    "  for generated_answer, true_answer in zip(generated_answers, true_answers):\n",
    "    if is_correct(generated_answer, true_answer):\n",
    "      correct_count += 1\n",
    "  return correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X3hdSBuIRQ-"
   },
   "outputs": [],
   "source": [
    "def eval_pass_at_n(questions, answers, model, tokenizer, n_solutions = 16, problems_per_batch=4, temperature=0.7):\n",
    "  solved_count = 0\n",
    "\n",
    "  for i in tqdm(range(0, len(questions), problems_per_batch)):\n",
    "    question_batch = questions[i: i + problems_per_batch]\n",
    "    answer_batch = answers[i: i + problems_per_batch]\n",
    "\n",
    "    extracted_gold_answer_batch = [extract_final_answer(answer) for answer in answer_batch]\n",
    "\n",
    "    question_prompt_batch = list(map(lambda q: q_prompt(q), question_batch))\n",
    "\n",
    "    tokenized_batch = tokenizer(question_prompt_batch, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    outputs = generate_with_sampling(model, tokenized_batch, temperature, n_solutions)\n",
    "\n",
    "    generated_answers = tokenizer.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    extracted_answer_batch = [extract_final_answer(answer) for answer in generated_answers]\n",
    "\n",
    "    extracted_answer_batched = np.array(extracted_answer_batch).reshape(-1, n_solutions).tolist()\n",
    "\n",
    "    # Calculate how many problems were solved at least once\n",
    "    correctness = [gold_answer in generated_answers for gold_answer, generated_answers in zip(extracted_gold_answer_batch, extracted_answer_batched)]\n",
    "\n",
    "    solved_count += sum(correctness)\n",
    "\n",
    "  return solved_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yAPC08A9EI8"
   },
   "outputs": [],
   "source": [
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, model, eval_dataloader, **kwargs):\n",
    "\n",
    "      print(f\"epoch={state.epoch}\")\n",
    "\n",
    "      if (state.epoch is not None):\n",
    "        questions = dataset[\"test\"][\"question\"]\n",
    "        true_answers = dataset[\"test\"][\"answer\"]\n",
    "\n",
    "        print(f\"evaluating score@1...\")\n",
    "        correct_count = eval_pass_at_1(model, questions, true_answers)\n",
    "\n",
    "        n_solutions = 16\n",
    "        print(f\"evaluating score@{n_solutions}...\")\n",
    "\n",
    "        correct_count_at_n = eval_pass_at_n(questions, true_answers, model, tokenizer, n_solutions=n_solutions)\n",
    "\n",
    "        print(f\"evaluating score@{n_solutions}...2\")\n",
    "        correct_count_at_n_2 = eval_pass_at_n(questions, true_answers, model, tokenizer, n_solutions=n_solutions)\n",
    "\n",
    "        custom_metrics = {\n",
    "          \"eval_solve_rate@1\": correct_count,\n",
    "          \"eval_solve_rate@1_%\": correct_count / len(questions),\n",
    "\n",
    "          \"eval_solve_rate@n\": correct_count_at_n,\n",
    "          \"eval_solve_rate@n_%\": correct_count_at_n / len(questions),\n",
    "\n",
    "          \"eval_solve_rate@n_2\": correct_count_at_n_2,\n",
    "          \"eval_solve_rate@n_2_%\": correct_count_at_n_2 / len(questions),\n",
    "        }\n",
    "\n",
    "        trainer.log(custom_metrics)\n",
    "\n",
    "        metrics.update(custom_metrics)\n",
    "\n",
    "      print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeWqxNbOKFkU"
   },
   "source": [
    "#Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uu2oNGp6YG_"
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdEm_a1zXW4Y"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.05,\n",
    "    eval_strategy=\"steps\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    args = args,\n",
    "    data_collator = collator,\n",
    "    callbacks=[PrinterCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxGbggAXXW6t",
    "outputId": "9ea4197e-4ccf-4c38-d7a8-de43b657502b"
   },
   "outputs": [],
   "source": [
    "train_dataloader = trainer.get_train_dataloader()\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KW6I0vUYGNi",
    "outputId": "32863496-611e-42b0-b5bc-a26a94a02464"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VefsR_IFHLta",
    "outputId": "00868a7c-43ce-4717-8d56-8ef8b4251464"
   },
   "outputs": [],
   "source": [
    "for input_id, label in zip(batch[\"input_ids\"][0], batch[\"labels\"][0]):\n",
    "  print(input_id.item(), label.item(), tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "D6udH5tCHL9L",
    "outputId": "b0add941-cef5-47ea-84f9-f2c85d3c9ac3"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "4a8ffcccd125477c9b13b8e4e898b223",
      "44d1deace86a4c38974905edcd47032b",
      "9ec7ba36be434f86bd2350d428785b10",
      "3c308758c41b4c2cbf5693825caeb6da",
      "cbdc0730b47d436693f295138bd4c7e3",
      "681820b9e705493195d499c2dec4e4cb",
      "655e3ebc751a4d64a1844211727a8f3f",
      "9dca96c14e3049568488230fddc0504e",
      "db419f9405d24d24948ee055d5a674a3",
      "4f703f0b5ffe4ebf8f029b23ba6f8f18",
      "ddad187c8487450cb7fc227e6cba4276",
      "2c93a54f34454af2a1a02ed6a528b3e3",
      "06f82d768dbd4b22b4bc58367eef307b",
      "50b038080c2f4a0e85b4206635f7e02c",
      "f56515776f1e4cae9bded27b79c9da5a",
      "d0d6632be76b4c84a860cce922951ca5",
      "ffebea33edbb425993325f7c779a8896",
      "310a20f1ca994bc399deb934a18dd07f",
      "370a00d73ca146a9b50fb65e7069242c",
      "428cb5f49f6d4022a3cb0fa22bed5354",
      "4b3a35aa49424ea9b2e4550a4bb7aa1b",
      "d94252759a1049d482c775590747ee8a",
      "fa67b9236b4946619a740ea2b4c76884",
      "239757819d7a4244b86016eb5e818fa9",
      "97c4d521b97b47b59ed0f684bd5f39f3",
      "33bcab2997f94d5b8f4f7c6f7d5ad68a",
      "60c2c48adc8649a9aaad37e852977df0",
      "1a7844a322cc420aa401e37e9e4b2ed1",
      "705e0b00e2574096a9103ce080268201",
      "f9d318af2164403280541782ed6eafef",
      "8cc8bb6c39474d50bf9bd03d3b5a3e92",
      "fb14ee74c9ac42b0beb7cfe8066afee7",
      "3567198d3afb44698358902dc8fbb3b4",
      "2c50b81a6e7d4da382c29a413d702477",
      "06131b4ba9264e1cb8abe832ff05d25a",
      "f067ab0d9f9b40a5abc0e981605ea92a",
      "f230d255208d442a9f88a1c201351d6f",
      "4537e53d500e4265a12c5f866ac125f8",
      "1121b3f6b9434c2687764b3287d33ad4",
      "752e79c78f7443ef883d2f5a48a2ba3f",
      "53e679b2f3464ea99b76b6b28c1aaf87",
      "01c6e0fb50d34310934d173f1e05face",
      "a419ee0146644f95a13877f985c8178c",
      "7afc6231f8684cfbbd305e79a249641a",
      "eb74ec465abf4222b37884f60ab19825",
      "d6efd7eae9b6402fb58ca7596384e58c",
      "12ea72a482aa4743a1e749c6f741a383",
      "b340be4c0f5d4d25a0835a7572d0d63e",
      "98c3100cdfb549a49f238b193f13e70e",
      "dcffa37018144498863b4830b31d6309",
      "7258ece3d1dd409dbe2f94ad9ceeeed1",
      "ef9d7e7c75c04478b6dac4f983b65973",
      "0271a0e73fdd4c53a3de0d5a2a8504a5",
      "b53d9b2704c54153b85aa763d13b17af",
      "ff67f3ea313f4d0c8570be183244a379",
      "a0345a6526cc411eb0cf5b18e146b201",
      "02e625c3c354487a8a4f9ac6e998b862",
      "e0d4a934375c4bd79543c4d7c3342751",
      "f044105b656d4784b4abe3591f758442",
      "7226bb29e4764ce9be4eeb23abe4c339",
      "212caff9271d4e00b20306df893a2237",
      "4e2437c0f52446a18e43b34caf17eab7",
      "8a370a9368d64acabce11a3f0db1c4db",
      "091044fa51244eb6bc2a3a530b06410c",
      "6afd093f241e45ecac85d4b9f5d58119",
      "2d6c66792a3e44f9936ec6ac858b8b81",
      "e79578993d81468087427bc01355de59",
      "b8784f5c05e74ad5a2ab72807b3a2c4a",
      "72aa2769e7f447268f1ab8ffa2d5c042",
      "0c68d8f65fb3496590ef263ffad4e100",
      "3475b3da33cc477f8d8cb63adc4b899d",
      "6485c47abae44d6995b1fe974aa11340",
      "fe0616b3360e4d59a35e36e14ee7ad25",
      "050a7108fd384c808f6ff386d4f67a4c",
      "481bbee52d434fa39ca332f72b1eb59c",
      "a60da1dddc4146fba4a285e5746b9353",
      "c6f7c2c9ef474f0e96efbeedc93d37d2",
      "6184ebe9cde04c888b5b80ca8350d309",
      "67d208a0df8740ab8908f2170bf33e22",
      "526220b96188482f982539866c4f75f4",
      "435676a59b1e4d75bdf09d66f04ca79b",
      "917f72edeac44304953123cf8218c810",
      "28f333ba06764813a683f174b68a1f30",
      "9e4c5b2f1feb4ca18c1cb6311f7e2a64",
      "989838006c4a4d9ab42b8af63eae3a68",
      "6742b807271b43b8a4b99fa2de0724ab",
      "c0cd7ca10b1d4e229d0fc1c51b89aa04"
     ]
    },
    "id": "aDcxyD4jkcuL",
    "outputId": "76c49c69-3bed-4724-90ea-5c5a237ee2c8"
   },
   "outputs": [],
   "source": [
    "# 128 test samples\n",
    "# 16 solutions per problem\n",
    "# 0.05 eval rate\n",
    "# T = 0.7\n",
    "training_results = trainer.train()\n",
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MWlvHMfUt_Ce"
   },
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "df = pd.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VjcJehIeuQ7B",
    "outputId": "451c1c87-eecf-4854-a73d-ff49974f1dd8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "df[df[\"loss\"].notna()].plot(\n",
    "    x=\"step\", y=\"loss\", ax=plt.gca(), label=\"train loss\"\n",
    ")\n",
    "\n",
    "df[df[\"eval_loss\"].notna()].plot(\n",
    "    x=\"step\", y=\"eval_loss\", ax=plt.gca(), label=\"eval loss\"\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Evaluation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "RUlWAcwiv5bG",
    "outputId": "cafb1811-0464-4478-9be2-4d6afdf0b508"
   },
   "outputs": [],
   "source": [
    "custom_df = df[df[\"eval_solve_rate@1\"].notna()]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(\n",
    "    custom_df[\"step\"],\n",
    "    custom_df[\"eval_solve_rate@1_%\"],\n",
    "    marker=\"o\",\n",
    "    label=\"solve rate @1\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    custom_df[\"step\"],\n",
    "    custom_df[\"eval_solve_rate@n_%\"],\n",
    "    marker=\"o\",\n",
    "    label=\"solve rate @n\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    custom_df[\"step\"],\n",
    "    custom_df[\"eval_solve_rate@n_2_%\"],\n",
    "    marker=\"o\",\n",
    "    label=\"solve rate @n (run 2)\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Solve Rate\")\n",
    "plt.title(\"Solve Rate vs Training Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bnF8fI9myahi"
   },
   "outputs": [],
   "source": [
    "def save_trained(save_directory, trainer, tokenizer):\n",
    "  trainer.save_model(save_directory)\n",
    "  model.config.save_pretrained(save_directory)\n",
    "  tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mRMnyEtHDira"
   },
   "outputs": [],
   "source": [
    "save_trained(MODEL_SAVE_PATH, trainer, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
